1. Importing Libraries:-
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')

2. Importing the Data:-
pd.set_option('display.max_columns',None)
data=pd.read_csv("cellphone.csv")
data.rename(columns={'blue':'bluetooth','fc':'front_camera','m_dep':'mobile_depth','mobile_wt':'mobile_weight','pc':'primary_camera','px_height':'pixel_height','px_width':'pixel_width','sc_h':'screen_height','sc_w':'screen_width'},inplace=True)

3. Finding the Null values:-
data.info()

4. General info about the Data:-
data.head()
data.tail()
data.describe()

5. Exploratory Data Analysis:-

    A. Univariant Analysis
import sweetviz as sv 
my_report=sv.analyze(data)
my_report.show_html()

    B. Bivariant Analysis
 plt.figure(figsize=(40,45), facecolor='white')

plotnumber = 1

for column in data:
    if plotnumber <= 10:
        plt.subplot(5, 2, plotnumber)
        sns.histplot(x=data[column], hue=data['price_range'])
        plt.xlabel(column, fontsize=30)  # Decreased the fontsize for better visualization
        plt.ylabel("price_range", fontsize=20, rotation=90)
        plt.xticks(rotation=45)
    plotnumber += 1

plt.tight_layout()

    C. Multi Variant Analysis
sns.pairplot(data)

6. Data Preprocessing and Feature Engineering:-
data.isnull().sum()
corr_data=data[['battery_power', 'bluetooth', 'clock_speed', 'dual_sim', 'front_camera',
       'four_g', 'int_memory', 'mobile_depth', 'mobile_weight', 'n_cores',
       'primary_camera', 'pixel_height', 'pixel_width', 'ram', 'screen_height',
       'screen_width', 'talk_time', 'three_g', 'touch_screen', 'wifi',
       'price_range']]
plt.figure(figsize=(20,20))
sns.heatmap(data.corr(),annot=True , linewidth=1)
plt.figure(figsize=(20,25),facecolor='white')
plotnumber=1

for column in data:
    if plotnumber<=9:
        ax=plt.subplot(3,3,plotnumber)
        sns.boxplot(data[column])
        plt.xlabel(column,fontsize=20)
        plt.ylabel('Count',fontsize=20)
    plotnumber+=1
plt.tight_layout()
# Function to replace outliers with median for a specific column
def replace_outliers_with_median(data, front_camera ):
    median = data[front_camera].median()
    std_dev = data[front_camera].std()
    threshold = 3 * std_dev
    data[front_camera] = data[front_camera].apply(lambda x: median if abs(x - median) > threshold else x)
    return data
# Replace outliers with median for a specific column
data = replace_outliers_with_median(data, 'front_camera')
print("DataFrame with outliers replaced by median:")
print(data)

7. Splitting the data  into Train and Test Split:-

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.preprocessing import MinMaxScaler

# Create an instance of MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the training data
X_train = scaler.fit_transform(X_train)

# Transform the test data using the scaler fitted on the training data
X_test = scaler.transform(X_test)
X=data.drop('price_range',axis=1)
y=data.price_range

8. Model Creation:-
  
  A. Linear Regression
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder

# Training the linear regression model
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

# Making predictions on the testing data
predictions = linear_reg.predict(X_test)

# Calculating mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:", r2)

  B. Decision Tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split


# Creating a Decision Tree Regressor
dt = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters like max_depth, min_samples_split, etc.

# Fitting the model on the training data
dt.fit(X_train, y_train)

# Making predictions on the testing data
predictions = dt.predict(X_test)

# Calculating mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:",r2)

  Hyperparameter Tuning for Decision Tree
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error


# Define the parameter grid
param_grid = {
    'max_depth': [3, 5, 7, None],  # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
}

# Create a Decision Tree Regressor
dt_regressor = DecisionTreeRegressor(random_state=42)

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model for predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:",r2)

  C. Support Vector Machine (SVM)
from sklearn.svm import SVR

from sklearn.metrics import mean_squared_error, r2_score



# Creating a Support Vector Machine (SVM) regression model
svm = SVR(kernel='linear')  # You can choose different kernels like 'linear', 'poly', 'rbf', etc.

# Training the model on the training data
svm.fit(X_train, y_train)

# Making predictions on the testing data
predictions = svm.predict(X_test)

# Calculating mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:",r2)

  Hyperparameter Tuning for SVR
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score


# Define the grid of hyperparameters to search
param_grid = {
    'kernel': ['linear', 'rbf'],  # Kernel type
    'C': [0.1, 1, 10, 100],        # Regularization parameter
    'gamma': ['scale', 'auto']     # Kernel coefficient for 'rbf' kernel
}

# Creating a Support Vector Machine (SVM) regression model
svm = SVR()

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model for predictions
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

# Calculate mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculate R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:",r2)

  D. Random Forest 
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming 'data' is your dataset with features and 'Price' is the target variable for a regression problem
# Assuming you have already split your data into 'X_train', 'X_test' features and 'y_train', 'y_test' target variables

# Creating a Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed

# Fitting the model on the training data
rf.fit(X_train, y_train)

# Making predictions on the testing data
predictions = rf.predict(X_test)

# Calculating mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:",r2)

  E. XG Boost 
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Assuming 'data' is your dataset with features and 'Price' is the target variable for a regression problem


# Training the model
params = {
    'objective': 'reg:squarederror',  # Use regression objective function
    'eval_metric': 'rmse'             # Use root mean squared error (RMSE) as evaluation metric
}
num_rounds = 100  # Number of boosting rounds
dtrain = xgb.DMatrix(X_train, label=y_train)
xg_reg = xgb.train(params, dtrain, num_boost_round=num_rounds)

# Making predictions on the testing data
dtest = xgb.DMatrix(X_test)
predictions = xg_reg.predict(dtest)

# Calculating mean squared error
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculating R-squared
r2 = r2_score(y_test, predictions)
print("R-squared:", r2)

  F. Gradient Boosting 
from sklearn.ensemble import GradientBoostingRegressor
gbm=GradientBoostingRegressor()
gbm.fit(X_train,y_train)
y_gbm=gbm.predict(X_test)
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_gbm)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_gbm)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared (R2) score
r2 = r2_score(y_test, y_gbm)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R-squared (R2) Score:", r2)

  Hyperparameter Tuning for Gradient Boost Method
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],  # Number of boosting stages to be run
    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree
    'max_depth': [3, 4, 5]  # Maximum depth of the individual estimators
}

# Create a Gradient Boosting Regressor
gbm = GradientBoostingRegressor()

# Perform Grid Search with cross-validation
grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model for predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Calculate R-squared
r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)

